# Isolation Forest Training Pipeline

This document details the offline training process for the anomaly detection model used in the E-Habitat simulation.

## 1. Overview of Isolation Forest
Isolation Forest is an unsupervised learning algorithm for anomaly detection. Unlike most anomaly detection methods that construct a profile of normal instances and identify deviations, Isolation Forest explicitly "isolates" anomalies. It uses binary trees (Isolation Trees) where anomalies are partitioned into shorter paths than normal observations, as they are "few and different."

## 2. Why Unsupervised Learning?
In the E-Habitat environment, we primarily generate "normal" baseline data. Real-world failures (anomalies) can take many formsâ€”some of which might not be known beforehand. Unsupervised learning allows us to:
- Detect deviations without requiring a labeled "anomaly" dataset.
- Generalize across different types of sensor failures or environmental shifts.
- Utilize the vast amounts of normal operational data generated by the simulation.

## 3. Contamination Parameter
The `contamination` parameter defines the expected proportion of outliers in the training data.
- **Value:** `0.01` (1%).
- **Implication:** The model expects roughly 1% of the baseline training data to be somewhat atypical (noise). This helps in setting the decision threshold for the anomaly score. During inference, observations with scores below this threshold are flagged as anomalies.

## 4. Training Workflow
1. **Data Loading:** The pipeline loads the `baseline_features.npy` file, which contains 48 hours of simulated normal telemetry.
2. **Feature Matrix:** The input is a 12-dimensional matrix (means, variances, and rates of change for 4 sensors).
3. **Model Fitting:** The algorithm builds 100 Isolation Trees using a random subset of the baseline data.
4. **Scoring:** The model learns the average path length for "normal" data points to establish a baseline threshold.

## 5. Artifact Generation (`.pkl`)
The trained model is serialized using `joblib` into `backend/ml/isolation_forest.pkl`. This binary format allows:
- **Fast Loading:** Efficient deserialization for real-time inference.
- **Persistent State:** The exact decision boundaries learned during training are preserved.

## 6. Why Model is Trained Offline
- **Resource Intensity:** Building 100 trees over 170k+ samples requires significant CPU time and memory, which would interfere with the simulation's real-time performance.
- **Stability:** Offline training ensures that the baseline model is stable and vetted before being used for live detection.
- **Reproducibility:** Training with a fixed `random_state` on a static `.npy` file ensures the resulting model is consistent.

## 7. Limitations
- **Memory Consumption:** As the number of trees (`n_estimators`) or training samples increases, the size of the `.pkl` file grows.
- **Sensitivity to Window Size:** The model's performance is tied to the 10-step sliding window used during feature extraction.
- **No Online Learning:** The model does not currently update its parameters as new "normal" data arrives during the live simulation.

## 8. Future Tuning Considerations
- **Hyperparameter Optimization:** Tuning `n_estimators` and `max_samples` to balance detection accuracy and inference speed.
- **Dynamic Contamination:** Adjusting the contamination threshold based on validation against known failure scenarios.
- **Feature Importance:** Analyzing which sensor features (e.g., Temperature Rate of Change vs. Humidity Variance) contribute most to anomaly scores.
